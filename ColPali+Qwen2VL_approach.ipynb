{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Install the packages"
      ],
      "metadata": {
        "id": "K4NXf6sRXSzO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1IQawOMGm69"
      },
      "outputs": [],
      "source": [
        "!pip install byaldi qwen_vl_utils gradio\n",
        "!python -m pip install git+https://github.com/huggingface/transformers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradio Application Implementation"
      ],
      "metadata": {
        "id": "pZYcfo8IXVvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
        "from PIL import Image\n",
        "import requests\n",
        "from byaldi import RAGMultiModalModel\n",
        "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import torch\n",
        "import re\n",
        "import base64\n",
        "\n",
        "RAG = RAGMultiModalModel.from_pretrained(\"vidore/colpali\", verbose=10)\n",
        "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "        \"Qwen/Qwen2-VL-2B-Instruct\",\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
        "\n",
        "def create_rag_index(image_path):\n",
        "    RAG.index(\n",
        "        input_path=image_path,\n",
        "        index_name=\"image_index\",\n",
        "        store_collection_with_index=True,\n",
        "        overwrite=True,\n",
        "    )\n",
        "\n",
        "def extract_relevant_text(qwen_output):\n",
        "    qwen_text = qwen_output[0]\n",
        "    lines = qwen_text.split('\\n')\n",
        "    relevant_text = []\n",
        "    for line in lines:\n",
        "        if re.match(r'[A-Za-z0-9]', line):\n",
        "            relevant_text.append(line.strip())\n",
        "    return \"\\n\".join(relevant_text)\n",
        "\n",
        "def ocr_image(image_path,text_query):\n",
        "    if text_query:\n",
        "      create_rag_index(image_path)\n",
        "      results = RAG.search(text_query, k=1, return_base64_results=True)\n",
        "\n",
        "      image_data = base64.b64decode(results[0].base64)\n",
        "      image = Image.open(BytesIO(image_data))\n",
        "    else:\n",
        "      image = Image.open(image_path)\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"image\",\n",
        "                    \"image\": image,\n",
        "                },\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": \"Extract all text present in the image.Try to identify all the texts.\"\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    text_prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
        "\n",
        "    inputs = processor(\n",
        "        text=[text_prompt],\n",
        "        images=[image],\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    inputs = inputs.to(device)\n",
        "\n",
        "    output_ids = model.generate(**inputs, max_new_tokens=1024)\n",
        "\n",
        "    generated_ids = [\n",
        "        output_ids[len(input_ids):]\n",
        "        for input_ids, output_ids in zip(inputs.input_ids, output_ids)\n",
        "    ]\n",
        "\n",
        "    output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "\n",
        "    relevant_text = extract_relevant_text(output_text)\n",
        "\n",
        "    return relevant_text\n",
        "\n",
        "\n",
        "def highlight_text(text, query):\n",
        "    highlighted_text = text\n",
        "    for word in query.split():\n",
        "        pattern = re.compile(re.escape(word), re.IGNORECASE)\n",
        "        highlighted_text = pattern.sub(lambda m: f'<span style=\"background-color: yellow;\">{m.group()}</span>', highlighted_text)\n",
        "    return highlighted_text\n",
        "\n",
        "def ocr_and_search(image, keyword):\n",
        "    extracted_text = ocr_image(image,keyword)\n",
        "    if keyword =='':\n",
        "      return extracted_text , 'Please Enter a Keyword'\n",
        "\n",
        "    else:\n",
        "      highlighted_text = highlight_text(extracted_text, keyword)\n",
        "    return extracted_text , highlighted_text\n",
        "\n",
        "interface = gr.Interface(\n",
        "    fn=ocr_and_search,\n",
        "    inputs=[\n",
        "        gr.Image(type=\"filepath\", label=\"Upload Image\"),\n",
        "        gr.Textbox(label=\"Enter Keyword\")\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Extracted Text\"),\n",
        "        gr.HTML(\"Search Result\"),\n",
        "    ],\n",
        "    title=\"OCR and Document Search Web Application\",\n",
        "    description=\"Upload an image to extract text in Hindi and English and search for keywords.\"\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    interface.launch(share=True)"
      ],
      "metadata": {
        "id": "9JCDwWiZGsCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation of the model locally"
      ],
      "metadata": {
        "id": "50q2lKTMXZMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
        "from PIL import Image\n",
        "import requests\n",
        "from byaldi import RAGMultiModalModel\n",
        "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import torch\n",
        "import re\n",
        "import base64\n",
        "\n",
        "RAG = RAGMultiModalModel.from_pretrained(\"vidore/colpali\")\n",
        "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "        \"Qwen/Qwen2-VL-2B-Instruct\",\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
        "image_path = \"path_to_image\"\n",
        "\n",
        "def create_rag_index(image_path):\n",
        "    RAG.index(\n",
        "        input_path=image_path,\n",
        "        index_name=\"image_index\",\n",
        "        store_collection_with_index=True,\n",
        "        overwrite=True,\n",
        "    )\n",
        "\n",
        "create_rag_index(image_path)\n",
        "\n",
        "text_query ='your_query'\n",
        "\n",
        "if text_query:\n",
        "    results = RAG.search(text_query, k=1, return_base64_results=True)\n",
        "\n",
        "    image_data = base64.b64decode(results[0].base64)\n",
        "    image = Image.open(BytesIO(image_data))\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "         \"content\": [\n",
        "            {\n",
        "                \"type\": \"image\",\n",
        "                \"image\": image,\n",
        "            },\n",
        "            {\n",
        "                \"type\": \"text\",\n",
        "                \"text\": \"Extract all the texts from the image.\" # prompt for Qwen2VL7BInstruct\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "text_prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
        "\n",
        "inputs = processor(\n",
        "    text = [text_prompt],\n",
        "    images = [image],\n",
        "    padding = True,\n",
        "    return_tensors = \"pt\"\n",
        ")\n",
        "\n",
        "inputs = inputs.to(\"cuda\")\n",
        "\n",
        "output_ids = model.generate(**inputs, max_new_tokens=50)\n",
        "\n",
        "generated_ids = [\n",
        "    output_ids[len(input_ids) :]\n",
        "    for input_ids, output_ids in zip(inputs.input_ids, output_ids)\n",
        "]\n",
        "\n",
        "output_text = processor.batch_decode(\n",
        "    generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
        ")\n",
        "\n",
        "print(output_text)"
      ],
      "metadata": {
        "id": "3eIehJrFLg91"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}